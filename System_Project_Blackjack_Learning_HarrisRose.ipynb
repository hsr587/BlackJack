{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='Introduction'></a>\n",
    "# Introduction\n",
    "\n",
    "In Assignment 10 for our class, we looked at the use of Q-Learning (QL) and Deep Reinforcement Learning (DRL) through application to Blackjack (BJ). A Medium article, entitled \"Blackjack, Stocks, and Reinforcement Learning: ML for Risk Management\" [1] (\"the Article\") was provided for us to review.  This article and the assignment, while useful for learning, left certain issues unaddressed and the purpose of this project is to better evaluate these algorthms as applied to BJ.\n",
    "\n",
    "The following issues with the article were noted:\n",
    "1. The results obtained seem unrealistic. For QL, the author demonstrated a reward per episode of -68/100, or -68%. For DRL, the author demonstrated an expected return of 643%/100 episode, or +6.43% per episode. Blackjack is a well studied game that is believed to have an optimal strategy and these values are not consistent with that prior analysis.  Without counting cards or other cheating methods, optimally played BJ has a player expected value of about -0.5% [3], which implies a house advantage.  Even by counting cards or other cheats,  player EV can only be brought to +1.0% [4].  This implies that the results presented in our assignemnt may have been erroneous.\n",
    "2. The Open AI Gym BJ environment has some shortcomings for teaching RL.  First, the output of the environment does not reveal player or dealer cards, making understanding the workings of the model challenging.  Second, the environment only offers a simplified game of BlackJack, which does not have Splits, Doubling Down, or Surrenders incorporated.  This simplified model is helpful for implementing RL tests but the missing options reduce the player EV.\n",
    "3. The assigned article output is challenging to interpret for a few reasons.  First the many images of BJ games are hard to understand and dominate the output.  Second, the author does not compare his alogithms to any baseline, so performance is difficult to interpret.\n",
    "\n",
    "\n",
    "To address these concerns, I decided to provide the following:\n",
    "1. Reassessment of QL and DRL in the gym BJ environment, including:\n",
    "    1. Establish baseline perfomance of a naive and expert strategies.\n",
    "    1. Compare performance of the Article QL and DRL models to these baselines.\n",
    "    1. Create new QL and DRL models to see if this performance can be improved.\n",
    "1. Provide an additional implmentation of BJ, which allows for Double Down, Surrender, and Split actions\n",
    "    1. Assess performance of naive and expert models in this environment\n",
    "    1. Train and assess new QL and DRL models in this environment.\n",
    "1. Provide an analysis of issues with the Article and lessons learned.\n",
    "\n",
    "Hypotheses:\n",
    "1. The results presented in the article are incorrect.\n",
    "2. The results presented in the article can be improved upon with better models.\n",
    "3. For blackjack, QL models will perform better than DRL models\n",
    "4. AI models can approach but not exceed the performance of expert models in Blackjack.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6D Analysis\n",
    "\n",
    "## Decomposition\n",
    "\n",
    "Decomposition of the BJ problem addressed herein is the nature of the project.  The main hypothesis of the project is that QL will produce superior results to DRL as QL is a more exact technique that works in smaller state spaces such as BJ.  We have therefore tested both algorithms and present results.  Additioanlly, by redesigning the BJ environment to allow for a closer look inside the \"black box\", this may allow for more insights as to where the previous effort went wrong.\n",
    "\n",
    "## Domain Expertise\n",
    "\n",
    "There is a great deal of content published on expert blackjack strategies[5, 6](#References). Fr this analysis, this [strategy](https://en.wikipedia.org/wiki/Blackjack#Blackjack_strategy) was chosen as the expert to be used.  Other cohioces are available, but the gain in reward is likely very small and beyond the scope of this papare\n",
    "\n",
    "[test](#Introduction)\n",
    "\n",
    "## Data\n",
    "\n",
    "Data for this project comes from the game itself.  No external sources were required.\n",
    "\n",
    "## Design\n",
    "\n",
    "\n",
    "\n",
    "## Diagnosis\n",
    "\n",
    "One purpose of this project is diagnosis of the issues with the assigned article.  Comparision to Expert and naive baselines will allow for detection of\n",
    "\n",
    "## Deployment\n",
    "\n",
    "Although beyond the scope of this article, deployment of this model could be performed by allowing outside users to find optimal plays"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Naive and Expert model assessment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import utils\n",
    "import blackjack as bj\n",
    "\n",
    "\n",
    "game = bj.Blackjack()\n",
    "#utils.play_bj_random(game)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "outputs": [],
   "source": [
    "from gym_utils import BASIC_STRATEGY"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "BASIC_STRATEGY"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Q Learning Implementation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DRL Implementation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from blackjack import Blackjack\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "game = Blackjack()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "('H5', '6', 0, False)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.deal()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='References'></a>\n",
    "# References\n",
    "\n",
    "1. https://towardsdatascience.com/playing-blackjack-using-model-free-reinforcement-learning-in-google-colab-aa2041a2c13d\n",
    "1. https://trevormcguire.medium.com/blackjack-stocks-and-reinforcement-learning-ea4014115aeb\n",
    "1. Baldwin, Roger R. et al. “The Optimum Strategy in Blackjack.” Journal of the American Statistical Association 51 (1956): 429-439.\n",
    "2. https://wizardofodds.com/games/blackjack/card-counting/introduction/\n",
    "3. https://en.wikipedia.org/wiki/Blackjack#Blackjack_strategy\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Envrionment packages\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
